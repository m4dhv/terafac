{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP0zIDelaJj+VcsPwRJfdrC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m4dhv/terafac/blob/main/level_2_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**LEVEL 2: Intermediate Techniques (CIFAR-10)**"
      ],
      "metadata": {
        "id": "qrP1LZKcMTo9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Importing dependencies and performing augmentation"
      ],
      "metadata": {
        "id": "IWI-pldjXGFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "NUM_WORKERS = 2\n",
        "EPOCHS = 12\n",
        "\n",
        "mean = (0.4914, 0.4822, 0.4465)\n",
        "std  = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "\n",
        "train_transform_light = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "train_transform_strong = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(224, scale=(0.7, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "    transforms.RandomErasing(p=0.3, scale=(0.02, 0.2), ratio=(0.3, 3.3))\n",
        "])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "Pv7q99dnMTRT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "B7MYcavVMGAP"
      },
      "outputs": [],
      "source": [
        "def get_loaders(train_transform):\n",
        "    train_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=True, download=True, transform=train_transform\n",
        "    )\n",
        "    test_dataset = torchvision.datasets.CIFAR10(\n",
        "        root=\"./data\", train=False, download=True, transform=test_transform\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    return train_loader, test_loader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Model building and ResNet50 fine tuning"
      ],
      "metadata": {
        "id": "TrMzCWMMXWHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(num_classes=10, unfreeze_layer4=True):\n",
        "    model = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
        "\n",
        "    # Freeze everything first\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # Unfreeze last block for fine-tuning (important for 90%+)\n",
        "    if unfreeze_layer4:\n",
        "        for param in model.layer4.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    # Replace final classifier head\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "\n",
        "    return model.to(device)\n"
      ],
      "metadata": {
        "id": "7QWwdHjUMh_M"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Training and evaluation functions"
      ],
      "metadata": {
        "id": "fbTnjb2HXcDH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, loader, optimizer, criterion):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Training\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, 100.0 * correct / total\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0, 0, 0\n",
        "\n",
        "    for images, labels in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, pred = outputs.max(1)\n",
        "\n",
        "        correct += pred.eq(labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    return running_loss / total, 100.0 * correct / total\n"
      ],
      "metadata": {
        "id": "pfypqGSqMmPI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Experiment runner function for Ablation Study"
      ],
      "metadata": {
        "id": "EAWZuF1uXiyn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(exp_name, train_transform, lr=3e-4, weight_decay=1e-4, unfreeze_layer4=True):\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Running Experiment: {exp_name}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    train_loader, test_loader = get_loaders(train_transform)\n",
        "\n",
        "    model = build_model(unfreeze_layer4=unfreeze_layer4)\n",
        "\n",
        "    # label smoothing helps generalization\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "    # params to train\n",
        "    trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "    optimizer = optim.AdamW(trainable_params, lr=lr, weight_decay=weight_decay)\n",
        "\n",
        "    # cosine schedule = smoother improvements\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
        "\n",
        "    history = {\"train_loss\": [], \"train_acc\": [], \"test_loss\": [], \"test_acc\": []}\n",
        "\n",
        "    best_test_acc = 0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, optimizer, criterion)\n",
        "        te_loss, te_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_acc\"].append(tr_acc)\n",
        "        history[\"test_loss\"].append(te_loss)\n",
        "        history[\"test_acc\"].append(te_acc)\n",
        "\n",
        "        print(f\"Train Acc: {tr_acc:.2f}% | Test Acc: {te_acc:.2f}%\")\n",
        "\n",
        "        if te_acc > best_test_acc:\n",
        "            best_test_acc = te_acc\n",
        "            torch.save(model.state_dict(), f\"best_{exp_name}.pth\")\n",
        "            print(\"Saved best model\")\n",
        "\n",
        "    return best_test_acc, history\n"
      ],
      "metadata": {
        "id": "xBLD0154MqAd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Results of Ablation study"
      ],
      "metadata": {
        "id": "dOgXFK7PXqHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# 1) Without strong augmentation (LIGHT)\n",
        "acc_light, hist_light = run_experiment(\n",
        "    exp_name=\"light_aug\",\n",
        "    train_transform=train_transform_light,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    unfreeze_layer4=True\n",
        ")\n",
        "\n",
        "results.append([\"Light Augmentation\", acc_light])\n",
        "\n",
        "# 2) With strong augmentation (STRONG)\n",
        "acc_strong, hist_strong = run_experiment(\n",
        "    exp_name=\"strong_aug\",\n",
        "    train_transform=train_transform_strong,\n",
        "    lr=3e-4,\n",
        "    weight_decay=1e-4,\n",
        "    unfreeze_layer4=True\n",
        ")\n",
        "\n",
        "results.append([\"Strong Augmentation\", acc_strong])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6uAKnz3MuiZ",
        "outputId": "b5b5f810-51b6-460e-fdf7-8c7a83eb7b99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Running Experiment: light_aug\n",
            "==============================\n",
            "\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 84.63% | Test Acc: 89.16%\n",
            "Saved best model\n",
            "\n",
            "Epoch 2/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 93.84% | Test Acc: 91.08%\n",
            "Saved best model\n",
            "\n",
            "Epoch 3/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 96.58% | Test Acc: 90.97%\n",
            "\n",
            "Epoch 4/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 98.06% | Test Acc: 91.27%\n",
            "Saved best model\n",
            "\n",
            "Epoch 5/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 98.95% | Test Acc: 91.30%\n",
            "Saved best model\n",
            "\n",
            "Epoch 6/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.28% | Test Acc: 91.82%\n",
            "Saved best model\n",
            "\n",
            "Epoch 7/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.63% | Test Acc: 91.84%\n",
            "Saved best model\n",
            "\n",
            "Epoch 8/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.83% | Test Acc: 92.28%\n",
            "Saved best model\n",
            "\n",
            "Epoch 9/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.89% | Test Acc: 92.31%\n",
            "Saved best model\n",
            "\n",
            "Epoch 10/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.94% | Test Acc: 92.64%\n",
            "Saved best model\n",
            "\n",
            "Epoch 11/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.97% | Test Acc: 92.78%\n",
            "Saved best model\n",
            "\n",
            "Epoch 12/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 99.98% | Test Acc: 92.67%\n",
            "\n",
            "==============================\n",
            "Running Experiment: strong_aug\n",
            "==============================\n",
            "\n",
            "Epoch 1/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Acc: 80.59% | Test Acc: 91.05%\n",
            "Saved best model\n",
            "\n",
            "Epoch 2/12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  81%|████████▏ | 318/391 [03:09<00:43,  1.67it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Accuracy comparison table"
      ],
      "metadata": {
        "id": "t2sg5KahXvHP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(results, columns=[\"Experiment\", \"Best Test Accuracy (%)\"])\n",
        "df[\"Improvement (%)\"] = df[\"Best Test Accuracy (%)\"] - df.loc[0, \"Best Test Accuracy (%)\"]\n",
        "df\n"
      ],
      "metadata": {
        "id": "TP_6pN-WMycl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Training curves visualization"
      ],
      "metadata": {
        "id": "D8b8ak7lXzBP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_history(history, title=\"Training Curves\"):\n",
        "    epochs_range = range(1, EPOCHS+1)\n",
        "\n",
        "    plt.figure(figsize=(12,5))\n",
        "\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(epochs_range, history[\"train_loss\"], label=\"Train Loss\")\n",
        "    plt.plot(epochs_range, history[\"test_loss\"], label=\"Test Loss\")\n",
        "    plt.title(title + \" (Loss)\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(epochs_range, history[\"train_acc\"], label=\"Train Accuracy\")\n",
        "    plt.plot(epochs_range, history[\"test_acc\"], label=\"Test Accuracy\")\n",
        "    plt.title(title + \" (Accuracy)\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy (%)\")\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()\n",
        "    plt.savefig(\"results/level2_plots.png\")\n",
        "\n",
        "\n",
        "plot_history(hist_light, \"Light Augmentation\")\n",
        "plot_history(hist_strong, \"Strong Augmentation\")\n"
      ],
      "metadata": {
        "id": "aZ_-2k-FM18R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}